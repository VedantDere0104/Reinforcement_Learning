{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CrossEntropy_Method_On_CartPole.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkZf2RWIAeb_"
      },
      "source": [
        "####"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx_aVYhcEbnH"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch.nn.functional as nnf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysgIVuXQ2t0z"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clu2vz3f1ASj"
      },
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        \n",
        "        self.env_name = 'CartPole-v1'\n",
        "        self.batch_size = 200\n",
        "        self.hidden_dim = 128\n",
        "        self.target_reward_level = 199\n",
        "        self.lr = 0.01"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KJV7FmU1Wyq"
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels , \n",
        "                 hidden_dim , \n",
        "                 out_channels):\n",
        "        super(Network , self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(in_channels , hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_dim , out_channels)\n",
        "\n",
        "    def forward(self , x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6MHqC9Y1uRH"
      },
      "source": [
        "class Episode:\n",
        "    def __init__(self):\n",
        "        \n",
        "        self.total_reward = 0\n",
        "        \n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def add_step(self , state , action , reward):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.total_reward += reward"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6NdbZY12L1K"
      },
      "source": [
        "class Session:\n",
        "    def __init__(self , \n",
        "                 env , \n",
        "                 network, \n",
        "                 config = Config):\n",
        "        \n",
        "        self.env = env\n",
        "        self.config = Config()\n",
        "        self.number_of_training_games = self.config.batch_size\n",
        "        self.target_reward_level = self.config.target_reward_level\n",
        "        self.net = network\n",
        "\n",
        "        self.opt = torch.optim.Adam(self.net.parameters() , lr = self.config.lr)\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def get_batches(self):\n",
        "        \n",
        "        batch = []\n",
        "        episode = Episode()\n",
        "        state = self.env.reset()\n",
        "        number_of_actions = self.env.action_space.n\n",
        "\n",
        "        while True:\n",
        "            actions_score = self.net(torch.FloatTensor(state))\n",
        "            actions_prob = nnf.softmax(actions_score, dim=0)\n",
        "\n",
        "            action = np.random.choice(number_of_actions, size=1, p=actions_prob.data.numpy())[0]\n",
        "\n",
        "            new_state, reward, is_finished, _ = self.env.step(action)\n",
        "            episode.add_step(state, action, reward)\n",
        "\n",
        "            if is_finished:\n",
        "                if len(batch) == self.number_of_training_games:\n",
        "                    return batch\n",
        "                batch.append(episode)\n",
        "                new_state = self.env.reset()\n",
        "                episode = Episode()\n",
        "\n",
        "            state = new_state\n",
        "\n",
        "    def train(self):\n",
        "        step = 0\n",
        "\n",
        "        while True:\n",
        "            self.opt.zero_grad()\n",
        "            batch = self.get_batches()\n",
        "            mean_reward = np.array([episode.total_reward for episode in batch]).mean()\n",
        "            episode_state = []\n",
        "            episode_actions = []\n",
        "\n",
        "            for episode in batch:\n",
        "                \n",
        "                if episode.total_reward > mean_reward:\n",
        "                    episode_state.extend(episode.states)\n",
        "                    episode_actions.extend(episode.actions)\n",
        "\n",
        "            \n",
        "            predicted_actions_scores = self.net(torch.FloatTensor(episode_state))\n",
        "            episode_actions = torch.LongTensor(episode_actions)\n",
        "            loss = self.loss_fn(predicted_actions_scores , episode_actions)\n",
        "            loss.backward()\n",
        "            self.opt.step()\n",
        "\n",
        "            if mean_reward > self.target_reward_level:\n",
        "                print(f'Step {step} , loss {loss.item()} , reward {mean_reward} ')\n",
        "                print('Solved')\n",
        "                break\n",
        "            step += 1\n",
        "            print(f'Step {step} , loss {loss.item()} , reward {mean_reward} ')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhxZb96B8sdB"
      },
      "source": [
        "config = Config()\n",
        "env = gym.make(config.env_name)\n",
        "net = Network(env.observation_space.shape[0] , \n",
        "              config.hidden_dim , \n",
        "              env.action_space.n).to(device)\n",
        "session = Session(env , net)\n",
        "session.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BksdS8mh8_iT"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}